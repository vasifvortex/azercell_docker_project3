{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "30eab0b3-700e-46a7-a668-a7f0cbe02d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProcessingTask\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.repositories\", \"https://packages.confluent.io/maven/\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join([\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\",\n",
    "        \"org.apache.spark:spark-avro_2.12:3.5.0\"\n",
    "    ])) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"telcoaz\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"Telco12345\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"s3a://spark/yellow_tripdata_2024-01.parquet\")\n",
    "# pd_df = df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de4079c-a0a6-4f87-bcaa-9be4ae5c9815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|tolls_amount|\n",
      "+------------+\n",
      "|         0.0|\n",
      "|         0.0|\n",
      "|         0.0|\n",
      "|         0.0|\n",
      "|         0.0|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# working with selects\n",
    "df.selectExpr(\"tolls_amount\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1413fff-4b01-43ef-a5dd-4ba267071d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|tolls_amount|\n",
      "+------------+\n",
      "|         0.0|\n",
      "|         0.0|\n",
      "|         0.0|\n",
      "|         0.0|\n",
      "|         0.0|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#working with selects\n",
    "df.select(\"tolls_amount\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c180ad-0f86-443d-9fa3-c32b3cffbca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting only date part\n",
    "from pyspark.sql.functions import to_date\n",
    "new_df = df.withColumn(\"new_column\", to_date(\"tpep_pickup_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1bfd3c51-0cd6-4065-be7a-3840a4630bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|tpep_pickup_datetime|new_column|\n",
      "+--------------------+----------+\n",
      "|2024-01-01 00:57:55 |2024-01-01|\n",
      "|2024-01-01 00:03:00 |2024-01-01|\n",
      "|2024-01-01 00:17:06 |2024-01-01|\n",
      "|2024-01-01 00:36:38 |2024-01-01|\n",
      "|2024-01-01 00:46:51 |2024-01-01|\n",
      "|2024-01-01 00:54:08 |2024-01-01|\n",
      "|2024-01-01 00:49:44 |2024-01-01|\n",
      "|2024-01-01 00:30:40 |2024-01-01|\n",
      "|2024-01-01 00:26:01 |2024-01-01|\n",
      "|2024-01-01 00:28:08 |2024-01-01|\n",
      "|2024-01-01 00:35:22 |2024-01-01|\n",
      "|2024-01-01 00:25:00 |2024-01-01|\n",
      "|2024-01-01 00:35:16 |2024-01-01|\n",
      "|2024-01-01 00:43:27 |2024-01-01|\n",
      "|2024-01-01 00:51:53 |2024-01-01|\n",
      "|2024-01-01 00:50:09 |2024-01-01|\n",
      "|2024-01-01 00:41:06 |2024-01-01|\n",
      "|2024-01-01 00:52:09 |2024-01-01|\n",
      "|2024-01-01 00:56:38 |2024-01-01|\n",
      "|2024-01-01 00:32:34 |2024-01-01|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select(\"tpep_pickup_datetime\", \"new_column\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32e2f90b-eb79-4a85-9b1a-abca5db1cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|tpep_pickup_datetime|new_column|\n",
      "+--------------------+----------+\n",
      "| 2024-01-01 00:57:55|2024-01-01|\n",
      "| 2024-01-01 00:03:00|2024-01-01|\n",
      "| 2024-01-01 00:17:06|2024-01-01|\n",
      "| 2024-01-01 00:36:38|2024-01-01|\n",
      "| 2024-01-01 00:46:51|2024-01-01|\n",
      "| 2024-01-01 00:54:08|2024-01-01|\n",
      "| 2024-01-01 00:49:44|2024-01-01|\n",
      "| 2024-01-01 00:30:40|2024-01-01|\n",
      "| 2024-01-01 00:26:01|2024-01-01|\n",
      "| 2024-01-01 00:28:08|2024-01-01|\n",
      "| 2024-01-01 00:35:22|2024-01-01|\n",
      "| 2024-01-01 00:25:00|2024-01-01|\n",
      "| 2024-01-01 00:35:16|2024-01-01|\n",
      "| 2024-01-01 00:43:27|2024-01-01|\n",
      "| 2024-01-01 00:51:53|2024-01-01|\n",
      "| 2024-01-01 00:50:09|2024-01-01|\n",
      "| 2024-01-01 00:41:06|2024-01-01|\n",
      "| 2024-01-01 00:52:09|2024-01-01|\n",
      "| 2024-01-01 00:56:38|2024-01-01|\n",
      "| 2024-01-01 00:32:34|2024-01-01|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To extract pickup time from tpep_pickup_datetime column\n",
    "df2 = df.selectExpr(\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"to_date(tpep_pickup_datetime) as new_column\"\n",
    ")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get total passenger count by year with spark.sql \n",
    "df.createOrReplaceTempView(\"trips\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT year(tpep_pickup_datetime) AS year, SUM(passenger_count) AS total_passenger_count\n",
    "    FROM trips\n",
    "    GROUP BY year\n",
    "    ORDER BY year\n",
    "\"\"\")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e288df95-ab1b-41a4-bfe6-511d54a802ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+\n",
      "|pickup_year|total_passenger_count|\n",
      "+-----------+---------------------+\n",
      "|       2023|                   19|\n",
      "|       2009|                    4|\n",
      "|       2024|              3782723|\n",
      "|       2002|                    2|\n",
      "+-----------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, sum\n",
    "#group by year and find total_passenger_count by pickup year\n",
    "\n",
    "df2 = df.withColumn(\"pickup_year\", year(\"tpep_pickup_datetime\"))\n",
    "\n",
    "# 2. Group by year and sum passenger_count\n",
    "result = df2.groupBy(\"pickup_year\") \\\n",
    "            .agg(sum(\"passenger_count\").alias(\"total_passenger_count\"))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a55a15e9-eac9-4b48-bbf8-0327d47ddb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|average_tip_percentage|\n",
      "+----------------------+\n",
      "|   0.22809923154005327|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Calculate tip percentage (tip_amount / fare_amount)\n",
    "df_tip_pct = df.withColumn(\"tip_percentage\", col(\"tip_amount\") / col(\"fare_amount\"))\n",
    "\n",
    "# Average tip percentage across all trips\n",
    "df_tip_pct.agg(avg(\"tip_percentage\").alias(\"average_tip_percentage\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac1843d4-dc97-442a-ad7f-571927363b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|payment_type|  count|\n",
      "+------------+-------+\n",
      "|           1|2319046|\n",
      "|           2| 439191|\n",
      "|           0| 140162|\n",
      "|           4|  46628|\n",
      "|           3|  19597|\n",
      "+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count payment_types\n",
    "df.groupBy(\"payment_type\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f68f0404-b17e-4a01-acfe-4c55d3380ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|pickup_hour| count|\n",
      "+-----------+------+\n",
      "|          0| 79094|\n",
      "|          1| 53627|\n",
      "|          2| 37517|\n",
      "|          3| 24811|\n",
      "|          4| 16742|\n",
      "|          5| 18764|\n",
      "|          6| 41429|\n",
      "|          7| 83719|\n",
      "|          8|117209|\n",
      "|          9|128970|\n",
      "|         10|138778|\n",
      "|         11|150542|\n",
      "|         12|164559|\n",
      "|         13|169903|\n",
      "|         14|182898|\n",
      "|         15|189359|\n",
      "|         16|190201|\n",
      "|         17|206257|\n",
      "|         18|212788|\n",
      "|         19|184032|\n",
      "+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "\n",
    "# calculate pickup_hour and count which has most \n",
    "df.withColumn(\"pickup_hour\", hour(\"tpep_pickup_datetime\")) \\\n",
    "  .groupBy(\"pickup_hour\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"pickup_hour\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d5ed7c8-a9e5-46c8-89c1-7721cea5cbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|pickup_dayofweek| count|\n",
      "+----------------+------+\n",
      "|               1|339312|\n",
      "|               2|408277|\n",
      "|               3|463664|\n",
      "|               4|495032|\n",
      "|               5|428593|\n",
      "|               6|408588|\n",
      "|               7|421158|\n",
      "+----------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "\n",
    "# dayofweek: 1=Sunday, 2=Monday, ..., 7=Saturday\n",
    "df.withColumn(\"pickup_dayofweek\", dayofweek(\"tpep_pickup_datetime\")) \\\n",
    "  .groupBy(\"pickup_dayofweek\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"pickup_dayofweek\") \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bbfa488c-883e-4d33-9c5d-c404ea561d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|  trip_duration_min|\n",
      "+--------------------+---------------------+-------------------+\n",
      "| 2024-01-01 00:57:55|  2024-01-01 01:17:43|               19.8|\n",
      "| 2024-01-01 00:03:00|  2024-01-01 00:09:36|                6.6|\n",
      "| 2024-01-01 00:17:06|  2024-01-01 00:35:01| 17.916666666666668|\n",
      "| 2024-01-01 00:36:38|  2024-01-01 00:44:56|                8.3|\n",
      "| 2024-01-01 00:46:51|  2024-01-01 00:52:57|                6.1|\n",
      "| 2024-01-01 00:54:08|  2024-01-01 01:26:31|  32.38333333333333|\n",
      "| 2024-01-01 00:49:44|  2024-01-01 01:15:47|              26.05|\n",
      "| 2024-01-01 00:30:40|  2024-01-01 00:58:40|               28.0|\n",
      "| 2024-01-01 00:26:01|  2024-01-01 00:54:12| 28.183333333333334|\n",
      "| 2024-01-01 00:28:08|  2024-01-01 00:29:16| 1.1333333333333333|\n",
      "| 2024-01-01 00:35:22|  2024-01-01 00:41:41|  6.316666666666666|\n",
      "| 2024-01-01 00:25:00|  2024-01-01 00:34:03|               9.05|\n",
      "| 2024-01-01 00:35:16|  2024-01-01 01:11:52|               36.6|\n",
      "| 2024-01-01 00:43:27|  2024-01-01 00:47:11| 3.7333333333333334|\n",
      "| 2024-01-01 00:51:53|  2024-01-01 00:55:43| 3.8333333333333335|\n",
      "| 2024-01-01 00:50:09|  2024-01-01 01:03:57|               13.8|\n",
      "| 2024-01-01 00:41:06|  2024-01-01 00:53:42|               12.6|\n",
      "| 2024-01-01 00:52:09|  2024-01-01 00:52:28|0.31666666666666665|\n",
      "| 2024-01-01 00:56:38|  2024-01-01 01:03:17|               6.65|\n",
      "| 2024-01-01 00:32:34|  2024-01-01 00:49:33| 16.983333333333334|\n",
      "| 2024-01-01 00:52:30|  2024-01-01 00:57:37|  5.116666666666666|\n",
      "| 2024-01-01 00:36:30|  2024-01-01 01:13:53|  37.38333333333333|\n",
      "| 2024-01-01 00:44:24|  2024-01-01 00:51:57|               7.55|\n",
      "| 2024-01-01 00:14:29|  2024-01-01 00:14:29|                0.0|\n",
      "| 2024-01-01 00:42:05|  2024-01-01 01:16:49| 34.733333333333334|\n",
      "| 2024-01-01 00:12:35|  2024-01-01 00:19:21|  6.766666666666667|\n",
      "| 2024-01-01 00:20:11|  2024-01-01 00:42:53|               22.7|\n",
      "| 2024-01-01 00:44:01|  2024-01-01 00:54:31|               10.5|\n",
      "| 2024-01-01 00:08:12|  2024-01-01 00:41:08|  32.93333333333333|\n",
      "| 2024-01-01 00:36:25|  2024-01-01 00:47:26| 11.016666666666667|\n",
      "| 2024-01-01 00:49:31|  2024-01-01 01:35:41| 46.166666666666664|\n",
      "| 2024-01-01 00:15:34|  2024-01-01 00:28:51| 13.283333333333333|\n",
      "| 2024-01-01 00:15:26|  2024-01-01 00:33:57| 18.516666666666666|\n",
      "| 2024-01-01 00:18:36|  2024-01-01 00:40:49| 22.216666666666665|\n",
      "| 2024-01-01 00:09:17|  2024-01-01 00:16:55|  7.633333333333334|\n",
      "| 2024-01-01 00:21:40|  2024-01-01 00:27:05|  5.416666666666667|\n",
      "| 2024-01-01 00:39:03|  2024-01-01 00:44:28|  5.416666666666667|\n",
      "| 2024-01-01 00:45:51|  2024-01-01 00:49:43| 3.8666666666666667|\n",
      "| 2024-01-01 00:50:46|  2024-01-01 00:57:26|  6.666666666666667|\n",
      "| 2024-01-01 00:09:49|  2024-01-01 00:22:00| 12.183333333333334|\n",
      "| 2024-01-01 00:36:44|  2024-01-01 00:54:40| 17.933333333333334|\n",
      "| 2024-01-01 00:58:11|  2024-01-01 01:24:47|               26.6|\n",
      "| 2024-01-01 00:24:41|  2024-01-01 00:54:01| 29.333333333333332|\n",
      "| 2024-01-01 00:55:58|  2024-01-01 01:03:54|  7.933333333333334|\n",
      "| 2024-01-01 00:19:36|  2024-01-01 00:31:57|              12.35|\n",
      "| 2024-01-01 00:45:44|  2024-01-01 00:51:25|  5.683333333333334|\n",
      "| 2024-01-01 00:55:21|  2024-01-01 01:12:17| 16.933333333333334|\n",
      "| 2024-01-01 00:05:45|  2024-01-01 00:17:29| 11.733333333333333|\n",
      "| 2024-01-01 00:18:38|  2024-01-01 00:26:09|  7.516666666666667|\n",
      "| 2024-01-01 00:38:58|  2024-01-01 00:53:37|              14.65|\n",
      "+--------------------+---------------------+-------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp,col\n",
    "#to find trip duration in munutes\n",
    "df_duration = df.withColumn(\n",
    "    \"trip_duration_min\",\n",
    "    (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 60.0\n",
    ")\n",
    "df_duration.select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"trip_duration_min\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb507ead-163c-451d-8120-7b432ef8b8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------------------+\n",
      "|trip_distance|    trip_duration_hr|     avg_speed_mph|\n",
      "+-------------+--------------------+------------------+\n",
      "|         1.72|                0.33| 5.212121212121212|\n",
      "|          1.8|                0.11|16.363636363636363|\n",
      "|          4.7|  0.2986111111111111|15.739534883720932|\n",
      "|          1.4| 0.13833333333333334|10.120481927710843|\n",
      "|          0.8| 0.10166666666666667| 7.868852459016393|\n",
      "|          4.7|  0.5397222222222222| 8.708183221821926|\n",
      "|        10.82| 0.43416666666666665| 24.92130518234165|\n",
      "|          3.0|  0.4666666666666667| 6.428571428571429|\n",
      "|         5.44|  0.4697222222222222|11.581312832643407|\n",
      "|         0.04| 0.01888888888888889|2.1176470588235294|\n",
      "|         0.75| 0.10527777777777778|  7.12401055408971|\n",
      "|          1.2| 0.15083333333333335| 7.955801104972375|\n",
      "|          8.2|                0.61|13.442622950819672|\n",
      "|          0.4| 0.06222222222222222| 6.428571428571429|\n",
      "|          0.8| 0.06388888888888888|12.521739130434785|\n",
      "|          5.0|                0.23| 21.73913043478261|\n",
      "|          1.5|                0.21| 7.142857142857143|\n",
      "|          0.0|0.005277777777777778|               0.0|\n",
      "|          1.5| 0.11083333333333334|13.533834586466165|\n",
      "|         2.57| 0.28305555555555556| 9.079489695780175|\n",
      "|         0.66| 0.08527777777777777|  7.73941368078176|\n",
      "|          1.7|  0.6230555555555556|2.7284886312973695|\n",
      "|         0.94| 0.12583333333333332| 7.470198675496689|\n",
      "|          0.0|                 0.0|              NULL|\n",
      "|         23.9|  0.5788888888888889|41.285988483685216|\n",
      "|         1.08| 0.11277777777777778| 9.576354679802956|\n",
      "|         5.88| 0.37833333333333335|15.541850220264315|\n",
      "|         2.22|               0.175|12.685714285714287|\n",
      "|          5.1|  0.5488888888888889| 9.291497975708502|\n",
      "|         2.09|  0.1836111111111111|11.382753403933433|\n",
      "|         8.89|  0.7694444444444445|11.553790613718412|\n",
      "|          2.1| 0.22138888888888889| 9.485570890840654|\n",
      "|        11.51|  0.3086111111111111|37.296129612961295|\n",
      "|         1.79| 0.37027777777777776| 4.834208552138035|\n",
      "|          2.5|  0.1272222222222222|19.650655021834062|\n",
      "|          1.6| 0.09027777777777778|17.723076923076924|\n",
      "|          1.1| 0.09027777777777778|12.184615384615386|\n",
      "|          0.7| 0.06444444444444444| 10.86206896551724|\n",
      "|          1.1|  0.1111111111111111| 9.900000000000002|\n",
      "|         2.96| 0.20305555555555554|14.577291381668948|\n",
      "|          1.2| 0.29888888888888887| 4.014869888475837|\n",
      "|         2.42| 0.44333333333333336| 5.458646616541353|\n",
      "|         5.28|  0.4888888888888889|              10.8|\n",
      "|         1.47|  0.1322222222222222| 11.11764705882353|\n",
      "|         2.41| 0.20583333333333334|11.708502024291498|\n",
      "|         1.04| 0.09472222222222222|10.979472140762464|\n",
      "|         2.41|  0.2822222222222222|  8.53937007874016|\n",
      "|         1.06| 0.19555555555555557| 5.420454545454546|\n",
      "|         0.83| 0.12527777777777777| 6.625277161862528|\n",
      "|         1.94| 0.24416666666666667| 7.945392491467577|\n",
      "+-------------+--------------------+------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (unix_timestamp, col)\n",
    "\n",
    "#to find average speed in mph\n",
    "df_speed = df.withColumn(\n",
    "    \"trip_duration_hr\",\n",
    "    (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 3600.0\n",
    ").withColumn(\n",
    "    \"avg_speed_mph\",\n",
    "    col(\"trip_distance\") / col(\"trip_duration_hr\")\n",
    ")\n",
    "\n",
    "df_speed.select(\"trip_distance\", \"trip_duration_hr\", \"avg_speed_mph\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55776846-9537-456e-b230-84bc8df26e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|pickup_date|total_passengers|\n",
      "+-----------+----------------+\n",
      "| 2002-12-31|               2|\n",
      "| 2009-01-01|               4|\n",
      "| 2023-12-31|              19|\n",
      "| 2024-01-01|          109339|\n",
      "| 2024-01-02|          104215|\n",
      "| 2024-01-03|          111171|\n",
      "| 2024-01-04|          136003|\n",
      "| 2024-01-05|          137311|\n",
      "| 2024-01-06|          133978|\n",
      "| 2024-01-07|           92647|\n",
      "| 2024-01-08|          100966|\n",
      "| 2024-01-09|          110213|\n",
      "| 2024-01-10|          116838|\n",
      "| 2024-01-11|          130953|\n",
      "| 2024-01-12|          134404|\n",
      "| 2024-01-13|          144345|\n",
      "| 2024-01-14|          128141|\n",
      "| 2024-01-15|          101132|\n",
      "| 2024-01-16|          111709|\n",
      "| 2024-01-17|          131284|\n",
      "+-----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+\n",
      "|pickup_month|total_passengers|\n",
      "+------------+----------------+\n",
      "|           1|         3782720|\n",
      "|           2|               7|\n",
      "|          12|              21|\n",
      "+------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 108:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|pickup_year|total_passengers|\n",
      "+-----------+----------------+\n",
      "|       2002|               2|\n",
      "|       2009|               4|\n",
      "|       2023|              19|\n",
      "|       2024|         3782723|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# In order to find total_passengers per day, month, year\n",
    "# Per Day\n",
    "df.withColumn(\"pickup_date\", df.tpep_pickup_datetime.cast(\"date\")) \\\n",
    "  .groupBy(\"pickup_date\") \\\n",
    "  .agg(spark_sum(\"passenger_count\").alias(\"total_passengers\")) \\\n",
    "  .orderBy(\"pickup_date\") \\\n",
    "  .show()\n",
    "\n",
    "# Per Month\n",
    "df.withColumn(\"pickup_month\", month(\"tpep_pickup_datetime\")) \\\n",
    "  .groupBy(\"pickup_month\") \\\n",
    "  .agg(spark_sum(\"passenger_count\").alias(\"total_passengers\")) \\\n",
    "  .orderBy(\"pickup_month\") \\\n",
    "  .show()\n",
    "\n",
    "# Per Year\n",
    "df.withColumn(\"pickup_year\", year(\"tpep_pickup_datetime\")) \\\n",
    "  .groupBy(\"pickup_year\") \\\n",
    "  .agg(spark_sum(\"passenger_count\").alias(\"total_passengers\")) \\\n",
    "  .orderBy(\"pickup_year\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "109a5164-4804-4452-bf7c-3b3606fe3f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|VendorID|  count|\n",
      "+--------+-------+\n",
      "|       1| 729732|\n",
      "|       2|2234632|\n",
      "|       6|    260|\n",
      "+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# to find which vender has highest rides\n",
    "df.groupBy(\"VendorID\").count().orderBy(\"VendorID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59a619cd-a317-4877-b3d9-803649a09934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| avg_fare_per_trip|\n",
      "+------------------+\n",
      "|18.175061916792536|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# to find average_fare_per_trip\n",
    "df.agg(avg(\"fare_amount\").alias(\"avg_fare_per_trip\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80a09274-c283-43b2-8b4c-9e824db4ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|average_tip_percentage|\n",
      "+----------------------+\n",
      "|   0.22809923154005327|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Calculate tip percentage (tip_amount / fare_amount)\n",
    "df_tip_pct = df.withColumn(\"tip_percentage\", col(\"tip_amount\") / col(\"fare_amount\"))\n",
    "\n",
    "# Average tip percentage across all trips\n",
    "df_tip_pct.agg(avg(\"tip_percentage\").alias(\"average_tip_percentage\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd488fc8-ca56-4168-8d2f-ace1ed2772bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|passenger_count|  count|\n",
      "+---------------+-------+\n",
      "|           NULL| 140162|\n",
      "|              0|  31465|\n",
      "|              1|2188739|\n",
      "|              2| 405103|\n",
      "|              3|  91262|\n",
      "|              4|  51974|\n",
      "|              5|  33506|\n",
      "|              6|  22353|\n",
      "|              7|      8|\n",
      "|              8|     51|\n",
      "|              9|      1|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find passenger count per solo or group \n",
    "df.groupBy(\"passenger_count\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"passenger_count\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d17f336-f4b5-4a8f-93c8-47cb2b776f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|ride_type|  count|\n",
      "+---------+-------+\n",
      "|    group| 775885|\n",
      "|     solo|2188739|\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, count\n",
    "\n",
    "df_behavior = df.withColumn(\n",
    "    \"ride_type\",\n",
    "    when(df.passenger_count == 1, \"solo\").otherwise(\"group\")\n",
    ")\n",
    "\n",
    "df_behavior.groupBy(\"ride_type\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1da01e",
   "metadata": {},
   "source": [
    "Cache and Persist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4803832c",
   "metadata": {},
   "source": [
    "cache()\n",
    "Shortcut for persist() with the default storage level: MEMORY_ONLY.\n",
    "\n",
    "Keeps your DataFrame/RDD in RAM only.\n",
    "\n",
    "If data doesn’t fit in memory, Spark will recompute partitions that don’t fit when needed.\n",
    "\n",
    "persist()\n",
    "More flexible: lets you choose how and where your data is stored.\n",
    "\n",
    "You can use:\n",
    "\n",
    "MEMORY_ONLY (just like cache)\n",
    "\n",
    "MEMORY_AND_DISK (if RAM is full, spill to disk)\n",
    "\n",
    "DISK_ONLY, MEMORY_ONLY_SER, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3acaafcb-7cdf-4b87-b88d-1d09efab0e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|payment_type|   sum(fare_amount)|\n",
      "+------------+-------------------+\n",
      "|           1|4.303553892000613E7|\n",
      "|           3| 132330.08999999988|\n",
      "|           2|  7846602.789999888|\n",
      "|           4|  62243.18999999998|\n",
      "|           0| 2805509.7700004894|\n",
      "+------------+-------------------+\n",
      "\n",
      "Without cache: 0.2492225170135498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 13:14:33 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|payment_type|   sum(fare_amount)|\n",
      "+------------+-------------------+\n",
      "|           1|4.303553892000613E7|\n",
      "|           3| 132330.08999999988|\n",
      "|           2|  7846602.789999888|\n",
      "|           4|  62243.18999999998|\n",
      "|           0| 2805509.7700004894|\n",
      "+------------+-------------------+\n",
      "\n",
      "With cache: 0.45485997200012207\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"s3a://spark/yellow_tripdata_2024-01.parquet\")\n",
    "\n",
    "# Expensive operation: group by and sum\n",
    "start = time.time()\n",
    "df.groupBy(\"payment_type\").sum(\"fare_amount\").show()\n",
    "print(\"Without cache:\", time.time() - start)\n",
    "\n",
    "# With cache\n",
    "df_cached = df.cache()\n",
    "df_cached.count()  # Triggers caching\n",
    "start = time.time()\n",
    "df_cached.groupBy(\"payment_type\").sum(\"fare_amount\").show()\n",
    "print(\"With cache:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b29ec",
   "metadata": {},
   "source": [
    "PartitionBy and write to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "96d4583c-6064-49f8-b4a6-be3ea6624dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repart = df.withColumn(\"pickup_date\", to_date(\"tpep_pickup_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d4cc1ab0-5d9f-4340-b9d8-9027254ab64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 267:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|pickup_date|\n",
      "+-----------+\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "| 2024-01-01|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_repart.select(\"pickup_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fae5c5-3515-44c1-ab3f-0cf95fbb2eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write partitioned data to s3\n",
    "df_repart.write \\\n",
    "    .partitionBy(\"pickup_date\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5124d3f5-66e9-42e3-97c7-a2859d306d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-06 19:30:22|  2024-01-06 19:40:17|              1|         1.48|         1|                 N|         114|         231|           1|       11.4|  0.0|    0.5|      2.77|         0.0|                  1.0|       18.17|                 2.5|        0.0|\n",
      "|       2| 2024-01-06 22:11:53|  2024-01-06 22:21:01|              2|         0.98|         1|                 N|         186|         100|           1|       10.0|  1.0|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       2| 2024-01-06 16:19:58|  2024-01-06 16:30:05|              1|         1.34|         1|                 N|         113|         170|           1|       10.7|  0.0|    0.5|      2.94|         0.0|                  1.0|       17.64|                 2.5|        0.0|\n",
      "|       2| 2024-01-06 11:43:41|  2024-01-06 11:58:37|              3|         2.96|         1|                 N|         230|         144|           2|       16.3|  0.0|    0.5|       0.0|         0.0|                  1.0|        20.3|                 2.5|        0.0|\n",
      "|       2| 2024-01-06 16:30:14|  2024-01-06 16:34:20|              2|         0.49|         1|                 N|         249|         114|           1|        4.4|  0.0|    0.5|      1.68|         0.0|                  1.0|       10.08|                 2.5|        0.0|\n",
      "|       1| 2024-01-06 11:31:24|  2024-01-06 11:34:21|              1|          0.3|         1|                 N|         262|         263|           2|        4.4|  2.5|    0.5|       0.0|         0.0|                  1.0|         8.4|                 2.5|        0.0|\n",
      "|       2| 2024-01-06 21:14:56|  2024-01-06 21:23:31|              1|         1.77|         1|                 N|         238|         236|           1|       10.7|  1.0|    0.5|      3.14|         0.0|                  1.0|       18.84|                 2.5|        0.0|\n",
      "|       1| 2024-01-06 20:05:30|  2024-01-06 20:09:18|              0|          0.8|         1|                 N|         151|         166|           1|        6.5|  1.0|    0.5|       2.0|         0.0|                  1.0|        11.0|                 0.0|        0.0|\n",
      "|       2| 2024-01-06 22:10:52|  2024-01-06 22:12:26|              3|         0.47|         1|                 N|         132|         132|           3|       -4.4| -1.0|   -0.5|       0.0|         0.0|                 -1.0|       -8.65|                 0.0|      -1.75|\n",
      "|       1| 2024-01-06 18:45:17|  2024-01-06 19:09:47|              1|          2.9|         1|                 N|         141|           7|           1|       21.2|  5.0|    0.5|       5.5|         0.0|                  1.0|        33.2|                 2.5|        0.0|\n",
      "|       2| 2024-01-06 01:01:52|  2024-01-06 01:09:04|              2|         1.41|         1|                 N|         234|         233|           1|        9.3|  1.0|    0.5|       2.0|         0.0|                  1.0|        16.3|                 2.5|        0.0|\n",
      "|       2| 2024-01-06 13:51:24|  2024-01-06 14:00:02|              1|         1.14|         1|                 N|          68|         230|           1|        9.3|  0.0|    0.5|       2.0|         0.0|                  1.0|        15.3|                 2.5|        0.0|\n",
      "|       2| 2024-01-06 10:55:42|  2024-01-06 11:00:38|              1|         0.83|         1|                 N|         170|         170|           1|        7.2|  0.0|    0.5|      2.24|         0.0|                  1.0|       13.44|                 2.5|        0.0|\n",
      "|       2| 2024-01-06 14:40:57|  2024-01-06 15:18:01|              1|        17.26|         2|                 N|         132|          79|           1|       70.0|  0.0|    0.5|       4.0|        6.94|                  1.0|       86.69|                 2.5|       1.75|\n",
      "|       2| 2024-01-06 23:28:29|  2024-01-07 00:00:33|              1|         8.42|         1|                 N|         143|         256|           1|       40.1|  1.0|    0.5|      9.02|         0.0|                  1.0|       54.12|                 2.5|        0.0|\n",
      "|       2| 2024-01-06 09:24:12|  2024-01-06 09:32:07|              1|         2.15|         1|                 N|         237|         164|           1|       11.4|  0.0|    0.5|      3.08|         0.0|                  1.0|       18.48|                 2.5|        0.0|\n",
      "|       2| 2024-01-06 17:34:37|  2024-01-06 17:49:08|              1|         2.23|         1|                 N|         239|         162|           1|       15.6|  0.0|    0.5|       1.0|         0.0|                  1.0|        20.6|                 2.5|        0.0|\n",
      "|       1| 2024-01-06 10:21:09|  2024-01-06 11:06:18|              1|         11.9|        99|                 N|          17|          41|           1|       41.5|  0.0|    0.5|       0.0|         0.0|                  1.0|        43.0|                 0.0|        0.0|\n",
      "|       1| 2024-01-06 15:13:08|  2024-01-06 15:21:21|              1|          1.0|         1|                 N|         233|         237|           1|        9.3|  2.5|    0.5|      2.65|         0.0|                  1.0|       15.95|                 2.5|        0.0|\n",
      "|       2| 2024-01-06 10:05:48|  2024-01-06 10:12:34|              1|         1.06|         1|                 N|         166|         151|           1|        8.6|  0.0|    0.5|      2.02|         0.0|                  1.0|       12.12|                 0.0|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading only one partition \n",
    "partition_path = \"s3a://spark/pickup_date=2024-01-06\"\n",
    "df_one_partition = spark.read.parquet(partition_path)\n",
    "df_one_partition.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0412c9",
   "metadata": {},
   "source": [
    "Spark UDF vs Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d0a33126-452f-49ec-b0c8-e3a3629056e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 250:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|trip_distance|trip_type|\n",
      "+-------------+---------+\n",
      "|         1.72|    short|\n",
      "|          1.8|    short|\n",
      "|          4.7|     long|\n",
      "|          1.4|    short|\n",
      "|          0.8|    short|\n",
      "|          4.7|     long|\n",
      "|        10.82|     long|\n",
      "|          3.0|     long|\n",
      "|         5.44|     long|\n",
      "|         0.04|    short|\n",
      "+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Spark UDF example\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def trip_type(distance):\n",
    "    return \"short\" if distance < 3 else \"long\"\n",
    "\n",
    "trip_type_udf = udf(trip_type, StringType())\n",
    "\n",
    "df_with_type = df.withColumn(\"trip_type\", trip_type_udf(df[\"trip_distance\"]))\n",
    "df_with_type.select(\"trip_distance\", \"trip_type\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb47459-766a-4ca1-86a7-a3237a7a4300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-20.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24020a0d-909e-4c97-a0aa-50c5e7d9471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|pickup_date|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0| 2024-01-01|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0| 2024-01-01|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0| 2024-01-01|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0| 2024-01-01|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0| 2024-01-01|\n",
      "|       1| 2024-01-01 00:54:08|  2024-01-01 01:26:31|              1|          4.7|         1|                 N|         148|         141|           1|       29.6|  3.5|    0.5|       6.9|         0.0|                  1.0|        41.5|                 2.5|        0.0| 2024-01-01|\n",
      "|       2| 2024-01-01 00:49:44|  2024-01-01 01:15:47|              2|        10.82|         1|                 N|         138|         181|           1|       45.7|  6.0|    0.5|      10.0|         0.0|                  1.0|       64.95|                 0.0|       1.75| 2024-01-01|\n",
      "|       1| 2024-01-01 00:30:40|  2024-01-01 00:58:40|              0|          3.0|         1|                 N|         246|         231|           2|       25.4|  3.5|    0.5|       0.0|         0.0|                  1.0|        30.4|                 2.5|        0.0| 2024-01-01|\n",
      "|       2| 2024-01-01 00:26:01|  2024-01-01 00:54:12|              1|         5.44|         1|                 N|         161|         261|           2|       31.0|  1.0|    0.5|       0.0|         0.0|                  1.0|        36.0|                 2.5|        0.0| 2024-01-01|\n",
      "|       2| 2024-01-01 00:28:08|  2024-01-01 00:29:16|              1|         0.04|         1|                 N|         113|         113|           2|        3.0|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.0|                 2.5|        0.0| 2024-01-01|\n",
      "|       2| 2024-01-01 00:35:22|  2024-01-01 00:41:41|              2|         0.75|         1|                 N|         107|         137|           1|        7.9|  1.0|    0.5|       0.0|         0.0|                  1.0|        12.9|                 2.5|        0.0| 2024-01-01|\n",
      "|       1| 2024-01-01 00:25:00|  2024-01-01 00:34:03|              2|          1.2|         1|                 N|         158|         246|           1|       14.9|  3.5|    0.5|      3.95|         0.0|                  1.0|       23.85|                 2.5|        0.0| 2024-01-01|\n",
      "|       1| 2024-01-01 00:35:16|  2024-01-01 01:11:52|              2|          8.2|         1|                 N|         246|         190|           1|       59.0|  3.5|    0.5|     14.15|        6.94|                  1.0|       85.09|                 2.5|        0.0| 2024-01-01|\n",
      "|       1| 2024-01-01 00:43:27|  2024-01-01 00:47:11|              2|          0.4|         1|                 N|          68|          90|           1|        5.8|  3.5|    0.5|      1.25|         0.0|                  1.0|       12.05|                 2.5|        0.0| 2024-01-01|\n",
      "|       1| 2024-01-01 00:51:53|  2024-01-01 00:55:43|              1|          0.8|         1|                 N|          90|          68|           2|        6.5|  3.5|    0.5|       0.0|         0.0|                  1.0|        11.5|                 2.5|        0.0| 2024-01-01|\n",
      "|       1| 2024-01-01 00:50:09|  2024-01-01 01:03:57|              1|          5.0|         1|                 N|         132|         216|           2|       21.2| 2.75|    0.5|       0.0|         0.0|                  1.0|       25.45|                 0.0|       1.75| 2024-01-01|\n",
      "|       1| 2024-01-01 00:41:06|  2024-01-01 00:53:42|              1|          1.5|         1|                 N|         164|          79|           1|       12.8|  3.5|    0.5|      4.45|         0.0|                  1.0|       22.25|                 2.5|        0.0| 2024-01-01|\n",
      "|       2| 2024-01-01 00:52:09|  2024-01-01 00:52:28|              1|          0.0|         1|                 N|         237|         237|           2|        3.0|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.0|                 2.5|        0.0| 2024-01-01|\n",
      "|       2| 2024-01-01 00:56:38|  2024-01-01 01:03:17|              1|          1.5|         1|                 N|         141|         263|           1|        9.3|  1.0|    0.5|       3.0|         0.0|                  1.0|        17.3|                 2.5|        0.0| 2024-01-01|\n",
      "|       2| 2024-01-01 00:32:34|  2024-01-01 00:49:33|              1|         2.57|         1|                 N|         161|         263|           1|       17.7|  1.0|    0.5|      10.0|         0.0|                  1.0|        32.7|                 2.5|        0.0| 2024-01-01|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pandas udf example\n",
    "# add pip install pyarrow on workers as well......\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def trip_type_pandas_udf(distance_series):\n",
    "    return distance_series.apply(lambda d: \"short\" if d < 3 else \"long\")\n",
    "\n",
    "df_with_type = df.withColumn(\"trip_type\", trip_type_pandas_udf(df[\"trip_distance\"]))\n",
    "df_with_type.select(\"trip_distance\", \"trip_type\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5e00c",
   "metadata": {},
   "source": [
    "Why Pandas UDFs are Faster than Regular UDFs\n",
    "1. Execution Model\n",
    "Regular (Scalar) UDFs:\n",
    "Process one row at a time.\n",
    "Spark sends each row from the JVM to Python, applies your function, and sends the result back.\n",
    "\n",
    "This is slow because there are millions of rows and lots of cross-language (JVM–Python) communication!\n",
    "\n",
    "Pandas UDFs (Vectorized UDFs):\n",
    "Process a whole batch of rows at once using Pandas Series (vectorized, column-wise).\n",
    "Spark sends big batches (as Arrow tables) to Python, applies your function to the entire batch, and sends all results back at once.\n",
    "\n",
    "This is much faster because it uses efficient in-memory Arrow serialization and reduces communication overhead!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acd44e",
   "metadata": {},
   "source": [
    "coalesce and repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repartition \n",
    "df_repart = df.withColumn(\"pickup_date\", to_date(\"tpep_pickup_datetime\"))\n",
    "df_repart = df_repart.repartition(15)\n",
    "print(\"Partitions after repartition:\", df_repart.rdd.getNumPartitions()) \n",
    "\n",
    "df_repart.write \\\n",
    "    .partitionBy(\"pickup_date\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition by column\n",
    "df_by_date = df_repart.repartition(10, \"pickup_date\")\n",
    "print(\"Number of partitions:\", df_by_date.rdd.getNumPartitions())\n",
    "\n",
    "df_by_date.write \\\n",
    "    .partitionBy(\"pickup_date\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9aaf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coalesce\n",
    "df_coalesce = df.withColumn(\"pickup_date\", to_date(\"tpep_pickup_datetime\"))\n",
    "df_coalesce = df_coalesce.coalesce(2)\n",
    "print(\"Num partitions:\", df_coalesce.rdd.getNumPartitions()) \n",
    "\n",
    "df_coalesce.write \\\n",
    "    .partitionBy(\"pickup_date\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://spark/\")\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3585fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"Flatten Nested JSON\").getOrCreate()\n",
    "\n",
    "# Read the JSON file with schema\n",
    "schema_str = \"\"\"\n",
    "id BIGINT,\n",
    "name STRING,\n",
    "address STRUCT<city:STRING, zipcode:STRING>,\n",
    "orders ARRAY<STRUCT<order_id:STRING, items:ARRAY<STRING>, total:DOUBLE>>,\n",
    "metadata STRUCT<preferences:STRUCT<newsletter:BOOLEAN, language:STRING>, last_login:STRING>\n",
    "\"\"\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .schema(schema_str) \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .json(\"input.json\")  # <-- Replace with your JSON file path\n",
    "\n",
    "# Step 1: Explode the `orders` array\n",
    "df_exploded_orders = df.select(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    col(\"address.city\").alias(\"city\"),\n",
    "    col(\"address.zipcode\").alias(\"zipcode\"),\n",
    "    explode(\"orders\").alias(\"order\"),\n",
    "    col(\"metadata.preferences.newsletter\").alias(\"newsletter\"),\n",
    "    col(\"metadata.preferences.language\").alias(\"language\"),\n",
    "    col(\"metadata.last_login\").alias(\"last_login\")\n",
    ")\n",
    "\n",
    "# Step 2: Explode the `items` array inside each order\n",
    "df_final = df_exploded_orders.select(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"city\",\n",
    "    \"zipcode\",\n",
    "    col(\"order.order_id\").alias(\"order_id\"),\n",
    "    explode(\"order.items\").alias(\"item\"),  # Explode each item\n",
    "    col(\"order.total\").alias(\"total\"),\n",
    "    \"newsletter\",\n",
    "    \"language\",\n",
    "    \"last_login\"\n",
    ")\n",
    "\n",
    "# Show the final flattened DataFrame\n",
    "df_final.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
